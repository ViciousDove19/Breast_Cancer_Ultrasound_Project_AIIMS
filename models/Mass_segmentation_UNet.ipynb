{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abdad674-7954-4fd5-8512-1ada2a3aebbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32mc:\\Users\\rautc\\miniforge3\\envs\\aln_env\\lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rautc\\miniforge3\\envs\\aln_env\\lib\\site-packages\\torchvision\\_meta_registrations.py:164\u001b[0m\n\u001b[0;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[0;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m         ),\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rautc\\miniforge3\\envs\\aln_env\\lib\\site-packages\\torch\\library.py:795\u001b[0m, in \u001b[0;36mregister\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister_kernel\u001b[39m(\n\u001b[0;32m    777\u001b[0m     op: _op_identifier,\n\u001b[0;32m    778\u001b[0m     device_types: device_types_t,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    782\u001b[0m     lib: Optional[Library] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    783\u001b[0m ):\n\u001b[0;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Register an implementation for a device type for this operator.\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \n\u001b[0;32m    786\u001b[0m \u001b[38;5;124;03m    Some valid device_types are: \"cpu\", \"cuda\", \"xla\", \"mps\", \"ipu\", \"xpu\".\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;124;03m    This API may be used as a decorator.\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;124;03m        op (str | OpOverload): The operator to register an impl to.\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;124;03m        device_types (None | str | Sequence[str]): The device_types to register an impl to.\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;124;03m            If None, we will register to all device types -- please only use\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;124;03m            this option if your implementation is truly device-type-agnostic.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;124;03m        func (Callable): The function to register as the implementation for\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m \u001b[38;5;124;03m            the given device types.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03m        lib (Optional[Library]): If provided, the lifetime of this registration\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \n\u001b[0;32m    798\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m        >>> import torch\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m        >>> from torch import Tensor\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;124;03m        >>> from torch.library import custom_op\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;124;03m        >>> import numpy as np\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m        >>> # Create a custom op that works on cpu\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m        >>> @custom_op(\"mylib::numpy_sin\", mutates_args=(), device_types=\"cpu\")\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m        >>> def numpy_sin(x: Tensor) -> Tensor:\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;124;03m        >>>     x_np = x.numpy()\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m        >>>     y_np = np.sin(x_np)\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03m        >>>     return torch.from_numpy(y_np)\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m        >>> # Add implementations for the cuda device\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.register_kernel(\"mylib::numpy_sin\", \"cuda\")\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;124;03m        >>> def _(x):\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;124;03m        >>>     x_np = x.cpu().numpy()\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;124;03m        >>>     y_np = np.sin(x_np)\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;124;03m        >>>     return torch.from_numpy(y_np).to(device=x.device)\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m        >>> x_cpu = torch.randn(3)\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m        >>> x_cuda = x_cpu.cuda()\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        >>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m        >>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \n\u001b[0;32m    824\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    827\u001b[0m         op, (\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39mOpOverload, torch\u001b[38;5;241m.\u001b[39m_library\u001b[38;5;241m.\u001b[39mcustom_ops\u001b[38;5;241m.\u001b[39mCustomOpDef)\n\u001b[0;32m    828\u001b[0m     ):\n\u001b[0;32m    829\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    830\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_kernel(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): got unexpected type for op: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(op)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\rautc\\miniforge3\\envs\\aln_env\\lib\\site-packages\\torch\\library.py:184\u001b[0m, in \u001b[0;36m_register_fake\u001b[1;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[0;32m    181\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39m_refresh_packet(packet)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_defs\u001b[38;5;241m.\u001b[39madd(qualname)\n\u001b[1;32m--> 184\u001b[0m _defs\u001b[38;5;241m.\u001b[39madd(qualname)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rautc\\miniforge3\\envs\\aln_env\\lib\\site-packages\\torch\\_library\\fake_impl.py:31\u001b[0m, in \u001b[0;36mregister\u001b[1;34m(self, func, source)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;129m@kernel\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mkernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to directly set kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister\u001b[39m(\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m, func: Callable, source: \u001b[38;5;28mstr\u001b[39m, lib, \u001b[38;5;241m*\u001b[39m, allow_override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RegistrationHandle:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple # For type hinting (best practice)\n",
    "import sys\n",
    "sys.path.append('../utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d74a07-04b2-4468-af38-2a1429f3105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radiomics_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0aa259-fe18-4680-901c-b290c5a16e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# You need to adjust these paths to match where your 'images' and 'masks' folders are located.\n",
    "ROOT_DIR = \"../data/mass_data/AIIMS_Delhi_Mass_Data/\" \n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images/\")\n",
    "MASK_DIR = os.path.join(ROOT_DIR, \"labels/\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4 # Reduced for segmentation due to high memory consumption\n",
    "IMAGE_SIZE = (256, 256) # Standardize all images to this size (must be powers of 2 for U-Net)\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efb01da-6e6d-4b1b-a1ea-b9bde2c1303e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee32292-cfe2-4420-b75a-9836f4e2af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n",
    "        \"\"\"\n",
    "        Initialization: Loads all file paths once to speed up access later.\n",
    "\n",
    "        Args:\n",
    "            image_dir (str): Path to the folder containing input images.\n",
    "            mask_dir (str): Path to the folder containing ground truth masks.\n",
    "            transform (callable, optional): Optional transforms for data augmentation.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        \n",
    "        # Literal: Lists all files in the image directory and sorts them.\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        \n",
    "        # Conceptual: We assume your mask files have the same base name as your image files \n",
    "        # (e.g., 'patient001.png' and 'patient001_mask.png'). This sorting is crucial \n",
    "        # to ensure that image[i] always corresponds to mask[i].\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "        \n",
    "        # If no custom transform is provided, we define a default one.\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                # Resize all images/masks to the required size.\n",
    "                transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                # Convert the PIL image to a PyTorch Tensor (and normalizes to [0, 1]).\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "            \n",
    "        # Expert Check: Ensure the number of images and masks match.\n",
    "        if len(self.images) != len(self.masks):\n",
    "            raise ValueError(f\"Mismatched files: Found {len(self.images)} images and {len(self.masks)} masks.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        The core function: loads, preprocesses, and returns a single sample.\n",
    "        \"\"\"\n",
    "        # 1. Path Construction\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[index])\n",
    "\n",
    "        # 2. Image Loading and Mode Conversion\n",
    "        # Ultrasound is typically grayscale. We force 'L' (Luminance) mode for the image.\n",
    "        image = read_as_grayscale(img_path)\n",
    "        image = torch.from_numpy(image)[None, ...]\n",
    "        # Masks are binary (0/1 or 0/255). We force 'L' mode and ensure it's not converted to RGB.\n",
    "        mask = read_as_grayscale(mask_path)\n",
    "        mask = torch.from_numpy(mask)[None, ...]\n",
    "\n",
    "        # 3. Applying Transformation\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        # 4. Critical Type and Range Conversion\n",
    "        \n",
    "        # Conceptual: Image data should be Float32 for model computation. \n",
    "        # The ToTensor() transform already handles this (0.0 to 1.0 range).\n",
    "        \n",
    "        # Conceptual: Mask data must also be Float32 in the range [0.0, 1.0] \n",
    "        # because the loss function (BCEWithLogitsLoss) expects floating-point ground truth.\n",
    "        # If your masks were loaded as 0/255 integers, ToTensor() automatically converts them \n",
    "        # to 0.0/1.0 floats.\n",
    "\n",
    "        # Ensure the mask remains binary after resizing/transformations (optional but safe)\n",
    "        mask[mask > 0.5] = 1.0\n",
    "        mask[mask <= 0.5] = 0.0\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d936488c-c946-4b36-ae1c-951c8949bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 310\n",
      "Batch Image Shape (B, C, H, W): torch.Size([4, 1, 256, 256])\n",
      "Batch Mask Shape (B, C, H, W): torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loader Initialization ---\n",
    "# 1. Create the Dataset instance\n",
    "train_dataset = BreastSegmentationDataset(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    mask_dir=MASK_DIR,\n",
    "    # Here you would typically add data augmentation transforms\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ToTensor(),\n",
    "        # Example Augmentation: transforms.RandomRotation(10),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "# Conceptual: The DataLoader is an iterable that batches and shuffles your data. \n",
    "# 'num_workers' uses separate processes to load data concurrently, preventing the CPU from \n",
    "# bottlenecking the GPU. Set this to a small number (e.g., 2 or 4) based on your CPU cores.\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0, \n",
    "    pin_memory=True # Transfers data from CPU to pinned memory, speeding up GPU transfer\n",
    ")\n",
    "\n",
    "print(f\"Total number of samples: {len(train_dataset)}\")\n",
    "# Sanity check the first batch\n",
    "data_sample, target_sample = next(iter(train_loader))\n",
    "print(f\"Batch Image Shape (B, C, H, W): {data_sample.shape}\") \n",
    "print(f\"Batch Mask Shape (B, C, H, W): {target_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae839887-9941-466d-b356-bb8cdb71b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the DoubleConv module for clarity and modularity\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        # PyTorch Sequential container groups operations into a single step\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            # Bias is often set to False when BatchNorm is used, as BN nullifies the bias effect.\n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True), # 'inplace=True' saves memory by modifying the input directly\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class ExpertUNet(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # We use a tuple to define the feature map progression\n",
    "        features = (64, 128, 256, 512) \n",
    "        \n",
    "        self.downs = nn.ModuleList() # Holds all the contraction blocks\n",
    "        self.ups = nn.ModuleList()   # Holds all the expansion blocks\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # --- ENCODER (Contracting Path) ---\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "            \n",
    "        # --- BOTTLENECK ---\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2) # 512 -> 1024\n",
    "        \n",
    "        # --- DECODER (Expanding Path) ---\n",
    "        for feature in reversed(features):\n",
    "            # 1. The Transposed Convolution (Upsampling)\n",
    "            # Literal: Half the feature maps (1024 -> 512, 512 -> 256, etc.)\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            # 2. The Double Convolution (Input is feature*2 because of concatenation)\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        # --- FINAL OUTPUT ---\n",
    "        # 1x1 Convolution maps 64 channels to 1 (for the binary mask)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        skip_connections = []\n",
    "\n",
    "        # 1. Run Downsampling blocks and collect skip connections\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x) # Store the high-res feature map\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # 2. Run Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Reverse the list so the highest-res skip connection is used first\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # 3. Run Upsampling blocks\n",
    "        # We iterate over the 'ups' list two items at a time (idx and idx+1)\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            # Step A: Upsampling using ConvTranspose2d\n",
    "            x = self.ups[idx](x) \n",
    "            \n",
    "            # Step B: Retrieve the corresponding skip connection\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            \n",
    "            # Expert Tip: Check if dimensions match. If they don't (rarely happens), \n",
    "            # you must crop the skip connection before concatenation.\n",
    "            \n",
    "            # Step C: Concatenation \n",
    "            # Literal: Stacking tensors along dimension 1 (the channel dimension).\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            \n",
    "            # Step D: Final DoubleConv layer\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Initialize and move model to device\n",
    "model = ExpertUNet(in_channels=1, out_channels=1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad622e7-a125-40c7-95fb-2bc087d0767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chinmay\\AppData\\Local\\Temp\\ipykernel_28676\\2587029763.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Expert Training Regime ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chinmay\\AppData\\Local\\Temp\\ipykernel_28676\\2587029763.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "# Loss Function: BCEWithLogitsLoss combines Sigmoid and Binary Cross Entropy\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# Optimizer: Adam is a solid default choice\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    \"\"\"Handles the training for one epoch.\"\"\"\n",
    "    \n",
    "    # Model is explicitly set to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Monitor running loss for the epoch\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # enumerate gives us the index (batch_idx) and the data (data, targets)\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Expert Tip: Using AMP (Automatic Mixed Precision) for faster training.\n",
    "        # This uses torch.float16 for many calculations, saving memory and speeding up GPU.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # 1. Forward Pass\n",
    "            predictions = model(data)\n",
    "            # 2. Calculate Loss\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # 3. Optimization Step (Zero Grad -> Backward -> Step)\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Scaler is used with AMP for stable gradient calculations\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Initialize the mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"--- Starting Expert Training Regime ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the epoch\n",
    "    epoch_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{NUM_EPOCHS} | Loss: {epoch_loss:.4f} | Time: {end_time - start_time:.2f}s\")\n",
    "    \n",
    "    # Conceptual: Here you would typically call a validation function and save the model\n",
    "    # if the validation performance (e.g., Dice Score) improved.\n",
    "    \n",
    "    # Example: torch.save(model.state_dict(), f\"unet_model_epoch{epoch+1}.pth\")\n",
    "    \n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bfdee-333a-4289-8c7c-fa5f1ccf1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328cd14-d01d-4393-bf35-e172ec682578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Same preprocessing as training (for a single image)\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),        # (1, H, W) for grayscale\n",
    "])\n",
    "\n",
    "def load_image_as_tensor(path: str) -> torch.Tensor:\n",
    "    \"\"\"Load a grayscale image and preprocess it for the model.\"\"\"\n",
    "    img = read_as_grayscale(path)      # grayscale PIL image\n",
    "    img = torch.from_numpy(img)[None, ...]\n",
    "    img = inference_transform(img)            # (1, H, W), float32 [0,1]\n",
    "    img = img.unsqueeze(0)                    # (1, 1, H, W) -> batch size 1\n",
    "    return img.to(DEVICE)\n",
    "\n",
    "\n",
    "def predict_mask(model: torch.nn.Module, image_path: str, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run the trained model on a single image and return:\n",
    "      - prob_mask: probability map tensor in [0,1], shape (H, W)\n",
    "      - bin_mask: binary mask tensor {0,1}, shape (H, W)\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Load and preprocess image\n",
    "    img = load_image_as_tensor(image_path)    # (1, 1, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 3. Forward pass -> logits\n",
    "        logits = model(img)                  # expected (1, 1, H, W)\n",
    "\n",
    "        # 4. Convert logits to probabilities with sigmoid\n",
    "        prob_mask = torch.sigmoid(logits)    # (1, 1, H, W)\n",
    "\n",
    "        # 5. Remove batch & channel dims -> (H, W)\n",
    "        prob_mask = prob_mask.squeeze(0).squeeze(0)\n",
    "\n",
    "        # 6. Threshold to get binary mask\n",
    "        bin_mask = (prob_mask >= threshold).float()  # (H, W), values 0 or 1\n",
    "\n",
    "    return prob_mask.cpu(), bin_mask.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97657e3-b9ac-4739-8137-ef6b7b4a6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_image_path = \"../data/mass_data/BrEaST-Lesions_USG-images_and_masks/images/case084.png\"\n",
    "#data\\mass_data\\BrEaST-Lesions_USG-images_and_masks\\images\n",
    "\n",
    "prob_mask, bin_mask = predict_mask(model, test_image_path, threshold=0.1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Input image\")\n",
    "plt.imshow(read_as_grayscale(test_image_path), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Predicted prob mask\")\n",
    "plt.imshow(prob_mask.numpy(), cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Binary mask\")\n",
    "plt.imshow(bin_mask.numpy(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dae0c3-b806-4f35-99ae-b7d442a2b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk = read_as_grayscale(\"../data/mass_data/BrEaST-Lesions_USG-images_and_masks/labels/case014_tumor.png\")\n",
    "imdisp(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66bf8e3c-b9e0-4406-b937-5e4d0ccba72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testerror(x):\n",
    "    if x < 10:\n",
    "        raise AttributeError(\"wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce8e61d2-9732-45d1-b55d-62c3a6dbfba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "wrong",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtesterror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m, in \u001b[0;36mtesterror\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtesterror\u001b[39m(x):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: wrong"
     ]
    }
   ],
   "source": [
    "testerror(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d2ada-deeb-4b01-8356-e5833074b32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aln_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
