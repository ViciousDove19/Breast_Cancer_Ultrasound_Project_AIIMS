{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdad674-7954-4fd5-8512-1ada2a3aebbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple # For type hinting (best practice)\n",
    "import sys\n",
    "sys.path.append('../utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d74a07-04b2-4468-af38-2a1429f3105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radiomics_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0aa259-fe18-4680-901c-b290c5a16e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# You need to adjust these paths to match where your 'images' and 'masks' folders are located.\n",
    "ROOT_DIR = \"../data/mass_data/AIIMS_Delhi_Mass_Data/\" \n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images/\")\n",
    "MASK_DIR = os.path.join(ROOT_DIR, \"labels/\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4 # Reduced for segmentation due to high memory consumption\n",
    "IMAGE_SIZE = (256, 256) # Standardize all images to this size (must be powers of 2 for U-Net)\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efb01da-6e6d-4b1b-a1ea-b9bde2c1303e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee32292-cfe2-4420-b75a-9836f4e2af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n",
    "        \"\"\"\n",
    "        Initialization: Loads all file paths once to speed up access later.\n",
    "\n",
    "        Args:\n",
    "            image_dir (str): Path to the folder containing input images.\n",
    "            mask_dir (str): Path to the folder containing ground truth masks.\n",
    "            transform (callable, optional): Optional transforms for data augmentation.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        \n",
    "        # Literal: Lists all files in the image directory and sorts them.\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        \n",
    "        # Conceptual: We assume your mask files have the same base name as your image files \n",
    "        # (e.g., 'patient001.png' and 'patient001_mask.png'). This sorting is crucial \n",
    "        # to ensure that image[i] always corresponds to mask[i].\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "        \n",
    "        # If no custom transform is provided, we define a default one.\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                # Resize all images/masks to the required size.\n",
    "                transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                # Convert the PIL image to a PyTorch Tensor (and normalizes to [0, 1]).\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "            \n",
    "        # Expert Check: Ensure the number of images and masks match.\n",
    "        if len(self.images) != len(self.masks):\n",
    "            raise ValueError(f\"Mismatched files: Found {len(self.images)} images and {len(self.masks)} masks.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        The core function: loads, preprocesses, and returns a single sample.\n",
    "        \"\"\"\n",
    "        # 1. Path Construction\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[index])\n",
    "\n",
    "        # 2. Image Loading and Mode Conversion\n",
    "        # Ultrasound is typically grayscale. We force 'L' (Luminance) mode for the image.\n",
    "        image = read_as_grayscale(img_path)\n",
    "        image = torch.from_numpy(image)[None, ...]\n",
    "        # Masks are binary (0/1 or 0/255). We force 'L' mode and ensure it's not converted to RGB.\n",
    "        mask = read_as_grayscale(mask_path)\n",
    "        mask = torch.from_numpy(mask)[None, ...]\n",
    "\n",
    "        # 3. Applying Transformation\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        # 4. Critical Type and Range Conversion\n",
    "        \n",
    "        # Conceptual: Image data should be Float32 for model computation. \n",
    "        # The ToTensor() transform already handles this (0.0 to 1.0 range).\n",
    "        \n",
    "        # Conceptual: Mask data must also be Float32 in the range [0.0, 1.0] \n",
    "        # because the loss function (BCEWithLogitsLoss) expects floating-point ground truth.\n",
    "        # If your masks were loaded as 0/255 integers, ToTensor() automatically converts them \n",
    "        # to 0.0/1.0 floats.\n",
    "\n",
    "        # Ensure the mask remains binary after resizing/transformations (optional but safe)\n",
    "        mask[mask > 0.5] = 1.0\n",
    "        mask[mask <= 0.5] = 0.0\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d936488c-c946-4b36-ae1c-951c8949bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 310\n",
      "Batch Image Shape (B, C, H, W): torch.Size([4, 1, 256, 256])\n",
      "Batch Mask Shape (B, C, H, W): torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loader Initialization ---\n",
    "# 1. Create the Dataset instance\n",
    "train_dataset = BreastSegmentationDataset(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    mask_dir=MASK_DIR,\n",
    "    # Here you would typically add data augmentation transforms\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ToTensor(),\n",
    "        # Example Augmentation: transforms.RandomRotation(10),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "# Conceptual: The DataLoader is an iterable that batches and shuffles your data. \n",
    "# 'num_workers' uses separate processes to load data concurrently, preventing the CPU from \n",
    "# bottlenecking the GPU. Set this to a small number (e.g., 2 or 4) based on your CPU cores.\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0, \n",
    "    pin_memory=True # Transfers data from CPU to pinned memory, speeding up GPU transfer\n",
    ")\n",
    "\n",
    "print(f\"Total number of samples: {len(train_dataset)}\")\n",
    "# Sanity check the first batch\n",
    "data_sample, target_sample = next(iter(train_loader))\n",
    "print(f\"Batch Image Shape (B, C, H, W): {data_sample.shape}\") \n",
    "print(f\"Batch Mask Shape (B, C, H, W): {target_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae839887-9941-466d-b356-bb8cdb71b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the DoubleConv module for clarity and modularity\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        # PyTorch Sequential container groups operations into a single step\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            # Bias is often set to False when BatchNorm is used, as BN nullifies the bias effect.\n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True), # 'inplace=True' saves memory by modifying the input directly\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class ExpertUNet(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # We use a tuple to define the feature map progression\n",
    "        features = (64, 128, 256, 512) \n",
    "        \n",
    "        self.downs = nn.ModuleList() # Holds all the contraction blocks\n",
    "        self.ups = nn.ModuleList()   # Holds all the expansion blocks\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # --- ENCODER (Contracting Path) ---\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "            \n",
    "        # --- BOTTLENECK ---\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2) # 512 -> 1024\n",
    "        \n",
    "        # --- DECODER (Expanding Path) ---\n",
    "        for feature in reversed(features):\n",
    "            # 1. The Transposed Convolution (Upsampling)\n",
    "            # Literal: Half the feature maps (1024 -> 512, 512 -> 256, etc.)\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            # 2. The Double Convolution (Input is feature*2 because of concatenation)\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        # --- FINAL OUTPUT ---\n",
    "        # 1x1 Convolution maps 64 channels to 1 (for the binary mask)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        skip_connections = []\n",
    "\n",
    "        # 1. Run Downsampling blocks and collect skip connections\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x) # Store the high-res feature map\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # 2. Run Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Reverse the list so the highest-res skip connection is used first\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # 3. Run Upsampling blocks\n",
    "        # We iterate over the 'ups' list two items at a time (idx and idx+1)\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            # Step A: Upsampling using ConvTranspose2d\n",
    "            x = self.ups[idx](x) \n",
    "            \n",
    "            # Step B: Retrieve the corresponding skip connection\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            \n",
    "            # Expert Tip: Check if dimensions match. If they don't (rarely happens), \n",
    "            # you must crop the skip connection before concatenation.\n",
    "            \n",
    "            # Step C: Concatenation \n",
    "            # Literal: Stacking tensors along dimension 1 (the channel dimension).\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            \n",
    "            # Step D: Final DoubleConv layer\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Initialize and move model to device\n",
    "model = ExpertUNet(in_channels=1, out_channels=1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad622e7-a125-40c7-95fb-2bc087d0767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chinmay\\AppData\\Local\\Temp\\ipykernel_28676\\2587029763.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Expert Training Regime ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chinmay\\AppData\\Local\\Temp\\ipykernel_28676\\2587029763.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "# Loss Function: BCEWithLogitsLoss combines Sigmoid and Binary Cross Entropy\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# Optimizer: Adam is a solid default choice\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    \"\"\"Handles the training for one epoch.\"\"\"\n",
    "    \n",
    "    # Model is explicitly set to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Monitor running loss for the epoch\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # enumerate gives us the index (batch_idx) and the data (data, targets)\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Expert Tip: Using AMP (Automatic Mixed Precision) for faster training.\n",
    "        # This uses torch.float16 for many calculations, saving memory and speeding up GPU.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # 1. Forward Pass\n",
    "            predictions = model(data)\n",
    "            # 2. Calculate Loss\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # 3. Optimization Step (Zero Grad -> Backward -> Step)\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Scaler is used with AMP for stable gradient calculations\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Initialize the mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"--- Starting Expert Training Regime ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the epoch\n",
    "    epoch_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{NUM_EPOCHS} | Loss: {epoch_loss:.4f} | Time: {end_time - start_time:.2f}s\")\n",
    "    \n",
    "    # Conceptual: Here you would typically call a validation function and save the model\n",
    "    # if the validation performance (e.g., Dice Score) improved.\n",
    "    \n",
    "    # Example: torch.save(model.state_dict(), f\"unet_model_epoch{epoch+1}.pth\")\n",
    "    \n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bfdee-333a-4289-8c7c-fa5f1ccf1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328cd14-d01d-4393-bf35-e172ec682578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Same preprocessing as training (for a single image)\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),        # (1, H, W) for grayscale\n",
    "])\n",
    "\n",
    "def load_image_as_tensor(path: str) -> torch.Tensor:\n",
    "    \"\"\"Load a grayscale image and preprocess it for the model.\"\"\"\n",
    "    img = read_as_grayscale(path)      # grayscale PIL image\n",
    "    img = torch.from_numpy(img)[None, ...]\n",
    "    img = inference_transform(img)            # (1, H, W), float32 [0,1]\n",
    "    img = img.unsqueeze(0)                    # (1, 1, H, W) -> batch size 1\n",
    "    return img.to(DEVICE)\n",
    "\n",
    "\n",
    "def predict_mask(model: torch.nn.Module, image_path: str, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run the trained model on a single image and return:\n",
    "      - prob_mask: probability map tensor in [0,1], shape (H, W)\n",
    "      - bin_mask: binary mask tensor {0,1}, shape (H, W)\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Load and preprocess image\n",
    "    img = load_image_as_tensor(image_path)    # (1, 1, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 3. Forward pass -> logits\n",
    "        logits = model(img)                  # expected (1, 1, H, W)\n",
    "\n",
    "        # 4. Convert logits to probabilities with sigmoid\n",
    "        prob_mask = torch.sigmoid(logits)    # (1, 1, H, W)\n",
    "\n",
    "        # 5. Remove batch & channel dims -> (H, W)\n",
    "        prob_mask = prob_mask.squeeze(0).squeeze(0)\n",
    "\n",
    "        # 6. Threshold to get binary mask\n",
    "        bin_mask = (prob_mask >= threshold).float()  # (H, W), values 0 or 1\n",
    "\n",
    "    return prob_mask.cpu(), bin_mask.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97657e3-b9ac-4739-8137-ef6b7b4a6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_image_path = \"../data/mass_data/BrEaST-Lesions_USG-images_and_masks/images/case084.png\"\n",
    "#data\\mass_data\\BrEaST-Lesions_USG-images_and_masks\\images\n",
    "\n",
    "prob_mask, bin_mask = predict_mask(model, test_image_path, threshold=0.1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Input image\")\n",
    "plt.imshow(read_as_grayscale(test_image_path), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Predicted prob mask\")\n",
    "plt.imshow(prob_mask.numpy(), cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Binary mask\")\n",
    "plt.imshow(bin_mask.numpy(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dae0c3-b806-4f35-99ae-b7d442a2b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk = read_as_grayscale(\"../data/mass_data/BrEaST-Lesions_USG-images_and_masks/labels/case014_tumor.png\")\n",
    "imdisp(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66bf8e3c-b9e0-4406-b937-5e4d0ccba72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testerror(x):\n",
    "    if x < 10:\n",
    "        raise AttributeError(\"wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce8e61d2-9732-45d1-b55d-62c3a6dbfba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "wrong",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtesterror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m, in \u001b[0;36mtesterror\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtesterror\u001b[39m(x):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: wrong"
     ]
    }
   ],
   "source": [
    "testerror(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d2ada-deeb-4b01-8356-e5833074b32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aln_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
