{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a25b4d-0408-4970-a3a4-a48e17e56229",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0dc29a8-43dd-4882-aca4-ba2d6dbe6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4956f8-d7a2-4bab-a96c-ba7003763ee3",
   "metadata": {},
   "source": [
    "## BrEaST Data Preprocessing into images and labels structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83b482a9-f4bc-46c8-b5e8-57c70b0b7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '../data/mass_data/BrEaST-Lesions_USG-images_and_masks/'\n",
    "lab_dir = '../data/mass_data/BrEaST-Lesions_USG-images_and_masks/labels/'\n",
    "img_dir = '../data/mass_data/BrEaST-Lesions_USG-images_and_masks/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51cc9307-fe2f-4cfb-9f86-ba901b77d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(path = src_dir):\n",
    "    tum_det = file.split('_')\n",
    "    if len(tum_det) > 1  and tum_det[1] == 'tumor.png':\n",
    "        if not os.path.exists(lab_dir + file):\n",
    "            shutil.copy(src_dir + file, lab_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21d06062-12b2-4361-9fa2-74321969c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(path = src_dir):\n",
    "    tum_det = file.split('_')\n",
    "    if len(tum_det) < 2 and file.lower().endswith('.png'):\n",
    "        if not os.path.exists(img_dir + file):\n",
    "            shutil.copy(src_dir + file, img_dir + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e475c-0a3d-4fe6-a735-3a1a2eccca91",
   "metadata": {},
   "source": [
    "## AIIMS Masses data processing for luminal vs non luminal categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be5d39c-a35a-4ce5-82f4-487e990e959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '../data/mass_data/AIIMS_Delhi_Mass_Data/images/'\n",
    "mask_dir = '../data/mass_data/AIIMS_Delhi_Mass_Data/labels/'\n",
    "dest_dir = '../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/images/'\n",
    "dest_masks_dir = '../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/masks/'\n",
    "labels = pd.read_csv('../data/mass_data/AIIMS_Delhi_Mass_Data/available_mass_images_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975ea70-04e7-4465-b2e6-4c600bf477c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eccbe57-122e-4063-acb0-138115c0dbf6",
   "metadata": {},
   "source": [
    "labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "541ba5da-dbb0-4587-b76b-9b9b410fb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.dropna(subset = ['ER'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c58ccb77-26e4-45fe-b4e3-4291ec5d46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['Luminal'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34083644-bf26-4ee6-b2b1-fbf2c9ae13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.loc[labels['ER'].str.startswith('POSITIVE'), 'Luminal'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d53d883-15bb-4c8b-bc27-34ad78b87cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['Patient_id'] = labels['Patient_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53bae0cd-5b8f-4bc3-971c-5dd288dea688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 == labels['Patient_id']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9740a99d-7fe1-4b12-9587-a2475b10aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in os.listdir(img_dir):\n",
    "    #print(file, file.split('.')[0].split('_')[0])\n",
    "    if (int(file.split('.')[0].split('_')[0]) == labels['Patient_id']).any():\n",
    "        #print(\"hi\", file)\n",
    "        if not os.path.exists(dest_dir + file):\n",
    "            shutil.copy(img_dir + file, dest_dir + file)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69fc8c4d-4bcd-483b-a6ec-86abe0d59549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/masks/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_masks_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ea3a354-eba0-4fa4-9980-ae35291614af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(mask_dir):\n",
    "    if (int(file.split('.')[0].split('_')[0]) == labels['Patient_id']).any():\n",
    "        if not os.path.exists(dest_masks_dir + file):\n",
    "            shutil.copy(mask_dir + file, dest_masks_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddc387db-7a40-4bc2-8c60-b8466e9fa592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba7b80fe-aa38-4ffc-8213-1612dc35f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/luminal_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d14488db-684b-4a72-863e-dfe7a5879315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = '../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/images/'\n",
    "ls = []\n",
    "for im in os.listdir(dest_dir):\n",
    "    sample = {}\n",
    "    sample['Patient_id'] = im.split('.')[0].split('_')[0]\n",
    "    #print(im.split('.')[0].split('_')[0])\n",
    "    sample['Image_file'] = im\n",
    "    ls.append(sample)\n",
    "    #print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1742666-9535-4c76-92db-0532f056b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdf = pd.DataFrame(ls)\n",
    "lsdf['Patient_id'] = lsdf['Patient_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bee52bfe-c676-4f1e-8e45-20d00ba6e78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsdf['Patient_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "081b66bc-bb36-467f-9417-0c512dd8eb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['Patient_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bdb660b-abaa-4167-86de-83e10e90673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = labels.merge(lsdf, on = 'Patient_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e4cdbec-8c36-4999-8134-3976f0905abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels.to_csv(\"../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/luminal_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a7a162f-2ebe-4fba-8d50-74b36c04cfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514150a0-85d8-4a75-848f-f47a3f28b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing masks to be the same size as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34b69b3-92c0-4b11-ab8e-e8567923f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 200, Resized masks: 200, Missing pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "img_dir = Path(\"../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/images\")\n",
    "lbl_dir = Path(\"../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/masks\")\n",
    "out_dir = Path(\"../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/resized_masks\")\n",
    "\n",
    "# Create output directory\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Allowed extensions\n",
    "img_exts = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"}\n",
    "lbl_exts = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"}\n",
    "\n",
    "# Build index for labels by stem (prefix without extension)\n",
    "label_index = {}\n",
    "for p in lbl_dir.iterdir():\n",
    "    if p.is_file() and p.suffix.lower() in lbl_exts:\n",
    "        label_index.setdefault(p.stem, []).append(p)\n",
    "\n",
    "def read_image_cv2(path):\n",
    "    # Read as BGR\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Failed to read image: {path}\")\n",
    "    return img\n",
    "\n",
    "def read_mask_cv2(path):\n",
    "    # Read as grayscale to preserve labels\n",
    "    mask = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    if mask is None:\n",
    "        raise RuntimeError(f\"Failed to read mask: {path}\")\n",
    "    return mask\n",
    "\n",
    "# Process images\n",
    "processed, missing, resized = 0, [], 0\n",
    "for img_path in img_dir.iterdir():\n",
    "    if not (img_path.is_file() and img_path.suffix.lower() in img_exts):\n",
    "        continue\n",
    "\n",
    "    stem = img_path.stem\n",
    "\n",
    "    # Find a label with the same prefix\n",
    "    lbl_path = None\n",
    "    if stem in label_index:\n",
    "        # Prefer .png if multiple, then by name\n",
    "        cands = sorted(label_index[stem], key=lambda x: (x.suffix.lower() != \".png\", x.name))\n",
    "        lbl_path = cands[0]\n",
    "\n",
    "    if lbl_path is None:\n",
    "        missing.append(stem)\n",
    "        continue\n",
    "\n",
    "    # Read image and assert size\n",
    "    img = read_image_cv2(img_path)\n",
    "    h, w = img.shape[:2]\n",
    "    assert (w, h) == (256, 256), f\"Image {img_path.name} is {w}x{h}, expected 256x256.\"\n",
    "\n",
    "    # Read mask\n",
    "    mask = read_mask_cv2(lbl_path)\n",
    "\n",
    "    # Resize mask if needed (nearest neighbor to preserve labels)\n",
    "    target_size = (w, h)\n",
    "    if (mask.shape[1], mask.shape[0]) != target_size:\n",
    "        mask_resized = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "    else:\n",
    "        mask_resized = mask\n",
    "\n",
    "    # Ensure output is uint8 (common for label masks)\n",
    "    if mask_resized.dtype != np.uint8:\n",
    "        mask_resized = mask_resized.astype(np.uint8)\n",
    "\n",
    "    # Save mask as PNG with same stem as image\n",
    "    out_path = out_dir / f\"{stem}.png\"\n",
    "    cv2.imwrite(str(out_path), mask_resized)\n",
    "\n",
    "    resized += 1\n",
    "    processed += 1\n",
    "\n",
    "print(f\"Processed: {processed}, Resized masks: {resized}, Missing pairs: {len(missing)}\")\n",
    "if missing:\n",
    "    print(\"No matching label for:\", \", \".join(missing[:20]) + (\" ...\" if len(missing) > 20 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece8c6a5-42a0-478b-a553-cd8580902657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def show_image_and_mask(image_path, mask_path, alpha=0.5, mask_color=(0, 0, 255)):\n",
    "    \"\"\"\n",
    "    Display:\n",
    "      - Original image\n",
    "      - Image with mask overlaid (mask as transparent colored overlay)\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the original image.\n",
    "        mask_path  (str or Path): Path to the corresponding mask.\n",
    "        alpha (float): Transparency of the mask overlay (0..1).\n",
    "        mask_color (tuple): BGR color for the mask overlay.\n",
    "    \"\"\"\n",
    "    # Read original image (BGR)\n",
    "    image = cv2.imread(str(image_path), cv2.IMREAD_COLOR)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not read image from {image_path}\")\n",
    "\n",
    "    # Read mask (grayscale or unchanged)\n",
    "    mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
    "    if mask is None:\n",
    "        raise ValueError(f\"Could not read mask from {mask_path}\")\n",
    "\n",
    "    # If mask has multiple channels, convert to single-channel\n",
    "    if mask.ndim == 3:\n",
    "        # Take one channel or convert to gray\n",
    "        mask_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        mask_gray = mask\n",
    "\n",
    "    # Ensure same spatial size\n",
    "    if image.shape[:2] != mask_gray.shape[:2]:\n",
    "        raise ValueError(\n",
    "            f\"Image and mask must have same size, got {image.shape[:2]} vs {mask_gray.shape[:2]}\"\n",
    "        )\n",
    "\n",
    "    # Create a 3-channel color mask for overlay\n",
    "    color_mask = np.zeros_like(image, dtype=np.uint8)\n",
    "    color_mask[:] = mask_color  # BGR\n",
    "\n",
    "    # Create boolean mask: where mask > 0 is foreground\n",
    "    # Adjust threshold as needed (e.g., > 127) depending on your masks\n",
    "    mask_binary = mask_gray > 0\n",
    "\n",
    "    # Prepare overlay image (copy of original)\n",
    "    overlay = image.copy()\n",
    "\n",
    "    # Blend only where mask is 1\n",
    "    overlay[mask_binary] = cv2.addWeighted(\n",
    "        image[mask_binary], 1 - alpha,\n",
    "        color_mask[mask_binary], alpha,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Show original and overlay\n",
    "    cv2.imshow(\"Original\", image)\n",
    "    cv2.imshow(\"Image + Mask Overlay\", overlay)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Optionally return the overlay for further use\n",
    "    return overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db1c25-d18b-4936-893b-cb886cc348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = show_image_and_mask(\"../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/images/2_1.tif\", \"../data/mass_data/AIIMS_Delhi_Mass_Data/subtype_classification/resized_masks/2_1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
